{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWDPES1eQfaUL1dvvGYgwG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HLokeshwari/ArtificialNeuralNetwork/blob/main/SEVENTH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "s_iZ1kU30GXz",
        "outputId": "fe0cb261-b076-4de1-8ebe-652fa2ea9ec6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "NeuralNetwork() takes no arguments",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-30e181f986c1>\u001b[0m in \u001b[0;36m<cell line: 87>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m# Create the neural network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m# Train the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: NeuralNetwork() takes no arguments"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Input features\n",
        "Y = iris.target  # Target labels\n",
        "\n",
        "# One-hot encoding of the target labels (for 3-class classification)\n",
        "encoder = OneHotEncoder()\n",
        "y = encoder.fit_transform(Y.reshape(-1, 1)).toarray()  # Convert to dense array\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the feature set\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Neural Network class implementing Backpropagation\n",
        "class NeuralNetwork:\n",
        "    def init(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
        "        # Initialize weights and biases\n",
        "        self.W1 = np.random.randn(input_size, hidden_size)\n",
        "        self.b1 = np.random.randn(hidden_size)\n",
        "        self.W2 = np.random.randn(hidden_size, output_size)\n",
        "        self.b2 = np.random.randn(output_size)\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    # Sigmoid activation function\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    # Derivative of the sigmoid function\n",
        "    def sigmoid_derivative(self, z):\n",
        "        return z * (1 - z)\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, X):\n",
        "        # Hidden layer\n",
        "        self.Z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.A1 = self.sigmoid(self.Z1)\n",
        "        # Output layer\n",
        "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
        "        self.A2 = self.sigmoid(self.Z2)\n",
        "        return self.A2\n",
        "\n",
        "    def backward(self, X, y, output):\n",
        "        # Calculate the error in the output\n",
        "        output_error = y - output  # (120, 3)\n",
        "        output_delta = output_error * self.sigmoid_derivative(output)  # Element-wise\n",
        "\n",
        "        # Calculate the error in the hidden layer\n",
        "        hidden_error = np.dot(output_delta, self.W2.T)  # (120, 5)\n",
        "        hidden_delta = hidden_error * self.sigmoid_derivative(self.A1)  # Element-wise\n",
        "\n",
        "        # Update the weights and biases\n",
        "        self.W2 += np.dot(self.A1.T, output_delta) * self.learning_rate  # (5, 3)\n",
        "        self.b2 += np.sum(output_delta, axis=0) * self.learning_rate  # (3,)\n",
        "        self.W1 += np.dot(X.T, hidden_delta) * self.learning_rate  # (4, 5)\n",
        "        self.b1 += np.sum(hidden_delta, axis=0) * self.learning_rate  # (5,)\n",
        "\n",
        "    # Train the network\n",
        "    def train(self, X, y, epochs=1000):\n",
        "        for epoch in range(epochs):\n",
        "            output = self.forward(X)\n",
        "            self.backward(X, y, output)\n",
        "            if (epoch + 1) % 100 == 0:\n",
        "                loss = np.mean(np.square(y - output))\n",
        "                print(f'Epoch {epoch + 1} - Loss: {loss}')\n",
        "\n",
        "    # Predict function\n",
        "    def predict(self, X):\n",
        "        output = self.forward(X)\n",
        "        return np.argmax(output, axis=1)\n",
        "\n",
        "# Neural Network configuration\n",
        "input_size = X_train.shape[1]  # 4 features\n",
        "hidden_size = 5  # Experiment with different hidden sizes\n",
        "output_size = 3  # 3 classes\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Create the neural network\n",
        "nn = NeuralNetwork(input_size, hidden_size, output_size, learning_rate)\n",
        "\n",
        "# Train the network\n",
        "nn.train(X_train, y_train, epochs=1000)\n",
        "\n",
        "# Predict the classes for the test set\n",
        "y_pred = nn.predict(X_test)\n",
        "\n",
        "# Convert one-hot encoded y_test back to class labels\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean(y_true == y_pred)\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Input features\n",
        "Y = iris.target  # Target labels\n",
        "\n",
        "# One-hot encoding of the target labels (for 3-class classification)\n",
        "encoder = OneHotEncoder()\n",
        "y = encoder.fit_transform(Y.reshape(-1, 1)).toarray()  # Convert to dense array\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the feature set\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Neural Network class implementing Backpropagation\n",
        "class NeuralNetwork:\n",
        "    def init(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
        "        # Initialize weights and biases\n",
        "        self.W1 = np.random.randn(input_size, hidden_size)\n",
        "        self.b1 = np.random.randn(hidden_size)\n",
        "        self.W2 = np.random.randn(hidden_size, output_size)\n",
        "        self.b2 = np.random.randn(output_size)\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    # Sigmoid activation function\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    # Derivative of the sigmoid function\n",
        "    def sigmoid_derivative(self, z):\n",
        "        return z * (1 - z)\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, X):\n",
        "        # Hidden layer\n",
        "        self.Z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.A1 = self.sigmoid(self.Z1)\n",
        "        # Output layer\n",
        "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
        "        self.A2 = self.sigmoid(self.Z2)\n",
        "        return self.A2\n",
        "\n",
        "    def backward(self, X, y, output):\n",
        "        # Calculate the error in the output\n",
        "        output_error = y - output  # (120, 3)\n",
        "        output_delta = output_error * self.sigmoid_derivative(output)  # Element-wise\n",
        "\n",
        "        # Calculate the error in the hidden layer\n",
        "        hidden_error = np.dot(output_delta, self.W2.T)  # (120, 5)\n",
        "        hidden_delta = hidden_error * self.sigmoid_derivative(self.A1)  # Element-wise\n",
        "\n",
        "        # Update the weights and biases\n",
        "        self.W2 += np.dot(self.A1.T, output_delta) * self.learning_rate  # (5, 3)\n",
        "        self.b2 += np.sum(output_delta, axis=0) * self.learning_rate  # (3,)\n",
        "        self.W1 += np.dot(X.T, hidden_delta) * self.learning_rate  # (4, 5)\n",
        "        self.b1 += np.sum(hidden_delta, axis=0) * self.learning_rate  # (5,)\n",
        "\n",
        "    # Train the network\n",
        "    def train(self, X, y, epochs=1000):\n",
        "        for epoch in range(epochs):\n",
        "            output = self.forward(X)\n",
        "            self.backward(X, y, output)\n",
        "            if (epoch + 1) % 100 == 0:\n",
        "                loss = np.mean(np.square(y - output))\n",
        "                print(f'Epoch {epoch + 1} - Loss: {loss}')\n",
        "\n",
        "    # Predict function\n",
        "    def predict(self, X):\n",
        "        output = self.forward(X)\n",
        "        return np.argmax(output, axis=1)\n",
        "\n",
        "# Neural Network configuration\n",
        "input_size = X_train.shape[1]  # 4 features\n",
        "hidden_size = 5  # Experiment with different hidden sizes\n",
        "output_size = 3  # 3 classes\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Create the neural network\n",
        "nn = NeuralNetwork(input_size, hidden_size, output_size, learning_rate)\n",
        "\n",
        "# Train the network\n",
        "nn.train(X_train, y_train, epochs=1000)\n",
        "\n",
        "# Predict the classes for the test set\n",
        "y_pred = nn.predict(X_test)\n",
        "\n",
        "# Convert one-hot encoded y_test back to class labels\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean(y_true == y_pred)\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "cctOsJ1U46U0",
        "outputId": "ac4c99a8-ed92-4cab-8d80-8830b88ebd1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "NeuralNetwork() takes no arguments",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-30e181f986c1>\u001b[0m in \u001b[0;36m<cell line: 87>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m# Create the neural network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m# Train the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: NeuralNetwork() takes no arguments"
          ]
        }
      ]
    }
  ]
}